{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view     ...      grade  \\\n",
       "0             1180      5650     1.0           0     0     ...          7   \n",
       "1             2570      7242     2.0           0     0     ...          7   \n",
       "2              770     10000     1.0           0     0     ...          6   \n",
       "3             1960      5000     1.0           0     0     ...          7   \n",
       "4             1680      8080     1.0           0     0     ...          8   \n",
       "5             5420    101930     1.0           0     0     ...         11   \n",
       "6             1715      6819     2.0           0     0     ...          7   \n",
       "7             1060      9711     1.0           0     0     ...          7   \n",
       "8             1780      7470     1.0           0     0     ...          7   \n",
       "9             1890      6560     2.0           0     0     ...          7   \n",
       "10            3560      9796     1.0           0     0     ...          8   \n",
       "11            1160      6000     1.0           0     0     ...          7   \n",
       "12            1430     19901     1.5           0     0     ...          7   \n",
       "13            1370      9680     1.0           0     0     ...          7   \n",
       "14            1810      4850     1.5           0     0     ...          7   \n",
       "15            2950      5000     2.0           0     3     ...          9   \n",
       "16            1890     14040     2.0           0     0     ...          7   \n",
       "17            1600      4300     1.5           0     0     ...          7   \n",
       "18            1200      9850     1.0           0     0     ...          7   \n",
       "19            1250      9774     1.0           0     0     ...          7   \n",
       "20            1620      4980     1.0           0     0     ...          7   \n",
       "21            3050     44867     1.0           0     4     ...          9   \n",
       "22            2270      6300     2.0           0     0     ...          8   \n",
       "23            1070      9643     1.0           0     0     ...          7   \n",
       "24            2450      6500     2.0           0     0     ...          8   \n",
       "25            1710      4697     1.5           0     0     ...          6   \n",
       "26            2450      2691     2.0           0     0     ...          8   \n",
       "27            1400      1581     1.5           0     0     ...          8   \n",
       "28            1520      6380     1.0           0     0     ...          7   \n",
       "29            2570      7173     2.0           0     0     ...          8   \n",
       "...            ...       ...     ...         ...   ...     ...        ...   \n",
       "21583          710      1157     2.0           0     0     ...          7   \n",
       "21584         1260       900     2.0           0     0     ...          7   \n",
       "21585         1870      5000     2.0           0     0     ...          7   \n",
       "21586         1430      1201     3.0           0     0     ...          8   \n",
       "21587         1520      1488     3.0           0     0     ...          8   \n",
       "21588         1210      1278     2.0           0     0     ...          8   \n",
       "21589         2540      4760     2.0           0     0     ...          8   \n",
       "21590         4910      9444     1.5           0     0     ...         11   \n",
       "21591         2770      3852     2.0           0     0     ...          8   \n",
       "21592         1190      1200     3.0           0     0     ...          8   \n",
       "21593         4170      8142     2.0           0     2     ...         10   \n",
       "21594         2500      5995     2.0           0     0     ...          8   \n",
       "21595         1530       981     3.0           0     0     ...          8   \n",
       "21596         3600      9437     2.0           0     0     ...          9   \n",
       "21597         3410     10125     2.0           0     0     ...         10   \n",
       "21598         3118      7866     2.0           0     2     ...          9   \n",
       "21599         3990      7838     2.0           0     0     ...          9   \n",
       "21600         4470      8088     2.0           0     0     ...         11   \n",
       "21601         1425      1179     3.0           0     0     ...          8   \n",
       "21602         1500     11968     1.0           0     0     ...          6   \n",
       "21603         2270      5536     2.0           0     0     ...          8   \n",
       "21604         1490      1126     3.0           0     0     ...          8   \n",
       "21605         2520      6023     2.0           0     0     ...          9   \n",
       "21606         3510      7200     2.0           0     0     ...          9   \n",
       "21607         1310      1294     2.0           0     0     ...          8   \n",
       "21608         1530      1131     3.0           0     0     ...          8   \n",
       "21609         2310      5813     2.0           0     0     ...          8   \n",
       "21610         1020      1350     2.0           0     0     ...          7   \n",
       "21611         1600      2388     2.0           0     0     ...          8   \n",
       "21612         1020      1076     2.0           0     0     ...          7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"]) #target value\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialing 3 dense layers, with 50 nodes fully multiplying in 2 layers, with activataion relu, \n",
    "#last node is output so linear\n",
    "input_len= len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation= 'relu'))\n",
    "model.add(keras.layers.Dense(50,activation= 'relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer= 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 1s 51us/step - loss: 352903175340.0320\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 237830811418.6240\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 98759910110.0032\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 76983980005.7856\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 74398965360.2304\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 72452511524.4544\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 70706044036.7104\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 69051148638.6176\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 67889043282.3296\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 66542067292.5696\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 65606852961.8944\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64786742162.2272\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64412850228.4288\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63824535302.9632\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63531923891.8144\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63289161508.4544\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63339027051.3152\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 63056768181.8624\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62749619873.3824\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62689934809.4976\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62529422013.2352\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62581308955.0336\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62233577232.7936\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62076307426.5088\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62242165148.8768\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61927335264.2560\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61993538938.4704\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61997148930.0480\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61882826974.8224\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61699263902.5152\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61975943013.9904\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61498697121.7920\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61423378989.0560\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61229719132.5696\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61209421244.0064\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 60870094526.8736\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61171045761.0240\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61006189074.8416\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60644240693.6576\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 60642277104.0256\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60537317189.2224\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 60422405134.7456\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 60261356483.3792\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60170328355.6352\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 60341438696.6528\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 59990791539.9168\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60108139482.3168\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60080422368.0512\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 59914073859.6864\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 59515297746.1248\n"
     ]
    }
   ],
   "source": [
    "#epochs is the # of times looking thru the data (hope it compiles in that time)\n",
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5390456021535236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds= model.predict(selected_feature_test)\n",
    "score(preds,price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val= selected_feature[18000:20000]\n",
    "price_val= price[18000:20000]\n",
    "\n",
    "selected_feature_test= selected_feature[20000:]\n",
    "price_test= price[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 59us/step - loss: 60230493882.1404\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49359783537.3227\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49357892568.8604\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49064923103.2320\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49340822150.2578\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49431256951.4667\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48976940262.2862\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49368135183.0187\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49086852690.3751\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49177578030.8764\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49118653433.6284\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48851137396.7360\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49015526419.1147\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49086259797.1058\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48974810611.7120\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49194288818.8587\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48943855830.8124\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48701143334.9120\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48942323713.8204\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48976321373.0702\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49046177083.8471\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48859665388.8853\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49219749833.3867\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48978946752.5120\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48873301215.0044\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 49165004743.5662\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48829864385.6498\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48975520898.1618\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48563698029.9093\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48706804478.4071\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48545743081.0169\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48764505496.6898\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48506961293.7671\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48716707139.1289\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48658952181.0773\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 48897509658.1689\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 49171968822.8409\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48656107128.6044\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48498436458.2684\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 48817312359.3102\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48840579168.4836\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48853722404.1813\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 48616703291.8471\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 48556336097.9627\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 48582900347.3351\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48529376360.6756\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48561423704.9742\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48288417054.7200\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 48517180234.8658\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 48597379496.1636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5558169860781627"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activation selu\n",
    "input_len= len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation= 'selu'))\n",
    "model.add(keras.layers.Dense(50,activation= 'selu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer= 'adam')\n",
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)\n",
    "\n",
    "val= model.predict(selected_feature_val)\n",
    "score(val,price_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 43us/step - loss: 81649450257.9769\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 54741760817.3796\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 54130617090.0480\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 54258689835.0080\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53719558207.7156\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 54086122784.5404\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 54211359497.3298\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 54277979486.4356\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53610041966.5920\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53531314844.1031\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53848126048.0284\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53979695941.4044\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53516008397.9378\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 53484016260.4373\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 53620660247.6658\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53295326563.8969\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53175040194.7876\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 57298767030.9547\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53217322372.6649\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52975351483.9609\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53349871950.9618\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53585060458.9511\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53532899304.3342\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 52714523645.2693\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 53246916518.7982\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 53357489473.3084\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 52650433748.0818\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52796457870.2222\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52882942976.9102\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52893580804.0960\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52882608124.3591\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 52277190925.4258\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52875464507.3920\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52112843855.1893: 0s - loss: 51623332637.64\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52382325786.3964\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 52079169880.0640\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52042148803.0151\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51738185913.6853\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 52211285397.0489\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 51879189839.8720\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51301558878.2080\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 51651014382.9333\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 51516907057.6071\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 51668892294.2578\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51558042754.1618\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51998805171.3138\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51017744354.8729\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 51201601008.0711\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50726118599.3387\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50952543349.4187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6034312424804318"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activation elu\n",
    "input_len= len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation= 'elu'))\n",
    "model.add(keras.layers.Dense(50,activation= 'elu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer= 'adam')\n",
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)\n",
    "\n",
    "val= model.predict(selected_feature_val)\n",
    "score(val,price_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 54us/step - loss: 61807270141.0418\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50944814022.6560\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 51255603553.1662\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50916969691.3636\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50907051411.2284\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 50385028877.8809\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50547888415.6302\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 52158042887.5093\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50615859449.4009\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 50319394672.1849\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49700989133.7102\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 50567418606.0231\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50413513415.7938\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 50765720471.3244\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 50028457019.1644\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 50162308770.4747\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50046196849.7778\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49729540755.0009\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49819059519.4880\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49842585967.7298\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 50222991398.2293\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49328240678.2293\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49188301970.5458\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49865936735.8009\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 50772445822.9760\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49517592342.9831\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 49257640802.5316\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49108929971.9964\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49856209238.2436\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49174089813.5609\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49006082042.5387\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 20us/step - loss: 49030051048.1067\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48764080739.6693\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48690438238.6631\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48764257987.2427\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48903167634.0907\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48605737808.3271\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49056069036.7147\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48830936186.8800\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 49558320942.6489\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48749699443.3707\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48846631898.6809\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48747135005.1271\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 48165877929.3013\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 48713337940.6507\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48415194145.6782\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 19us/step - loss: 48145968780.6293\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49382758627.5556\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 18us/step - loss: 49346650323.1716\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 17us/step - loss: 47994799587.3280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5740627211372606"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activation linear\n",
    "input_len= len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation= 'linear'))\n",
    "model.add(keras.layers.Dense(50,activation= 'linear'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer= 'adam')\n",
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)\n",
    "\n",
    "val= model.predict(selected_feature_val)\n",
    "score(val,price_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam',\n",
    "            activation='relu',\n",
    "            layers = [20,20],\n",
    "            nodes=50,\n",
    "            loss='mean_squared_error'):\n",
    "    model=keras.Sequential()\n",
    "    model.add(keras.layers.Dense(nodes, activation=activation))\n",
    "    model.add(keras.layers.Dense(nodes,activation=activation))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(loss=loss,optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 56us/step - loss: 416670513685.3902 - val_loss: 456355991781.3760\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416663985251.2142 - val_loss: 456349727064.0641\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416657991212.1458 - val_loss: 456343496687.6160\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416652148342.7840 - val_loss: 456337605001.2160\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416646625813.3902 - val_loss: 456331810832.3840\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416640896190.2365 - val_loss: 456325733023.7440\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416635257152.3983 - val_loss: 456319932563.4559\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416629767653.1484 - val_loss: 456314226212.8641\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416624333087.6302 - val_loss: 456308565737.4720\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416618886259.5982 - val_loss: 456302637088.7679\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416613051574.9547 - val_loss: 456296595456.0000\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416607369254.2293 - val_loss: 456290700099.5840\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416601782645.1911 - val_loss: 456284875259.9040\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416596247678.5209 - val_loss: 456279090266.1120\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416590736916.4800 - val_loss: 456273348263.9359\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416585249281.3653 - val_loss: 456267625922.5599\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416579775248.6116 - val_loss: 456261902794.7521\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416574316303.7014 - val_loss: 456256196444.1600\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416568864960.9671 - val_loss: 456250474627.0721\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416563423055.4169 - val_loss: 456244771160.0641\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416557988256.8818 - val_loss: 456239105966.0800\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416552562109.0987 - val_loss: 456233420587.0080\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416547133602.0196 - val_loss: 456227761422.3361\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416541714386.4889 - val_loss: 456222076043.2640\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416536296802.0764 - val_loss: 456216400363.5200\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416530875081.6142 - val_loss: 456210740674.5599\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416525466264.4622 - val_loss: 456205072072.7040\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416520053835.5485 - val_loss: 456199392722.9440\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416514644348.4729 - val_loss: 456193739325.4401\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416509231540.9067 - val_loss: 456188074131.4559\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416503825316.0675 - val_loss: 456182439608.3200\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416498419499.0080 - val_loss: 456176766025.7280\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416493019245.2267 - val_loss: 456171100045.3120\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416487612583.4808 - val_loss: 456165440356.3519\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416482204319.7440 - val_loss: 456159788269.5680\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416476800803.7262 - val_loss: 456154130153.4720\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416471402996.6223 - val_loss: 456148475445.2479\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416466004199.1965 - val_loss: 456142822047.7440\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 416460602518.1866 - val_loss: 456137180446.7199\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416455206662.5991 - val_loss: 456131547234.3040\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416449807049.6142 - val_loss: 456125876535.2960\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416444403912.2489 - val_loss: 456120234147.8400\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416439001677.8240 - val_loss: 456114566856.7040\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416433599909.4329 - val_loss: 456108915556.3519\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 416428198170.1689 - val_loss: 456103246168.0641\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 416422799401.8702 - val_loss: 456097601159.1681\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416417396118.8694 - val_loss: 456091953266.6880\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416412006933.3902 - val_loss: 456086295674.8800\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416406604407.6943 - val_loss: 456080649355.2640\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416401207357.8951 - val_loss: 456074985996.2880\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 62us/step - loss: 416662019331.4133 - val_loss: 456345165758.4640\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416652443487.8009 - val_loss: 456336573202.4320\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416644406880.9387 - val_loss: 456328251703.2960\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416636532686.8480 - val_loss: 456320066781.1840\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416628725892.8924 - val_loss: 456311908597.7599\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416620967275.1786 - val_loss: 456303821979.6481\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416613237172.9067 - val_loss: 456295731167.2321\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416605520585.6142 - val_loss: 456287677054.9760\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416597820629.9022 - val_loss: 456279623467.0080\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416590122217.9271 - val_loss: 456271545761.7920\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416582435660.6862 - val_loss: 456263514980.3519\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416574755773.5538 - val_loss: 456255466635.2640\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416567085614.8765 - val_loss: 456247446339.5840\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 13us/step - loss: 416559418601.9271 - val_loss: 456239406907.3920\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416551750161.7493 - val_loss: 456231403388.9279\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416544086877.0703 - val_loss: 456223366840.3200\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416536425048.7467 - val_loss: 456215349428.2240\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416528768375.9218 - val_loss: 456207346958.3361\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416521107188.3947 - val_loss: 456199319584.7679\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416513450282.5529 - val_loss: 456191304007.6800\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416505796435.0578 - val_loss: 456183291314.1760\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416498144073.0453 - val_loss: 456175314796.5441\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416490492788.7360 - val_loss: 456167249149.9520\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416482840776.2489 - val_loss: 456159269748.7360\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416475189084.1600 - val_loss: 456151284318.2080\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416467540829.0703 - val_loss: 456143254061.0560\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416459882932.9067 - val_loss: 456135232978.9440\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416452232638.9191 - val_loss: 456127227887.6160\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416444579199.2036 - val_loss: 456119237476.3519\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416436928380.9280 - val_loss: 456111199617.0240\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416429279252.0249 - val_loss: 456103192428.5441\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416421634200.9173 - val_loss: 456095206211.5840\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416413986790.5138 - val_loss: 456087203217.4080\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416406336583.9076 - val_loss: 456079175843.8400\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416398687134.6062 - val_loss: 456071172849.6639\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416391042316.5156 - val_loss: 456063169855.4880\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416383391177.8417 - val_loss: 456055148773.3760\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416375747379.2000 - val_loss: 456047171469.3120\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416368100027.0507 - val_loss: 456039163756.5441\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416360451189.4186 - val_loss: 456031154470.9120\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416352807070.3787 - val_loss: 456023138107.3920\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416345153688.9173 - val_loss: 456015157657.6000\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416337509773.7671 - val_loss: 456007134216.1920\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416329862654.6347 - val_loss: 455999108415.4880\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416322215914.1547 - val_loss: 455991138713.6000\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416314569057.1662 - val_loss: 455983119204.3519\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416306927268.2951 - val_loss: 455975117520.8960\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416299283265.7635 - val_loss: 455967093555.2000\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416291641709.9094 - val_loss: 455959111532.5441\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416283996396.6578 - val_loss: 455951138422.7840\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 60us/step - loss: 366716525448.7609 - val_loss: 343466963107.8400\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 268070694871.9502 - val_loss: 228709079711.7440\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 134704809883.8755 - val_loss: 94987289100.2880\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 77504531340.4018 - val_loss: 83261409198.0800\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 73812553039.8720 - val_loss: 80913445224.4480\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 71934196190.7769 - val_loss: 79354715832.3200\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 70382858880.7964 - val_loss: 78332683747.3280\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 69144982286.7911 - val_loss: 77236203290.6240\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 67931193911.9787 - val_loss: 76172524257.2800\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 66935079141.3760 - val_loss: 75249737662.4640\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 66124296843.7191 - val_loss: 75102243913.7280\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 65416098146.9867 - val_loss: 73772988301.3120\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 64769544097.3369 - val_loss: 73098140450.8160\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 64457863499.3209 - val_loss: 73076560822.2720\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 63973868619.5485 - val_loss: 72638819598.3360\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 63856429360.0142 - val_loss: 72240773398.5280\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63524614377.0169 - val_loss: 72309119778.8160\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63338423575.4382 - val_loss: 72137125789.6960\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 63356865289.3298 - val_loss: 71910476349.4400\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63349334816.9956 - val_loss: 71939982753.7920\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63063997644.8000 - val_loss: 71979352719.3600\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 63053450295.5236 - val_loss: 72174827864.0640\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62939698866.8587 - val_loss: 72783986360.3200\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63015797715.3991 - val_loss: 72222992367.6160\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62832052735.5449 - val_loss: 73051418001.4080\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62818197136.2702 - val_loss: 72065292566.5280\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 62894745153.9911 - val_loss: 72118163865.6000\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 13us/step - loss: 62844599883.0933 - val_loss: 72602594836.4800\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62888452085.0773 - val_loss: 71825045979.1360\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 63052298270.9476 - val_loss: 71988548403.2000\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62788228335.3884 - val_loss: 71855879880.7040\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62786772118.1867 - val_loss: 72220120252.4160\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62604678536.3058 - val_loss: 71993456459.7760\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62530301296.6400 - val_loss: 72691544752.1280\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62676467606.4142 - val_loss: 71867607547.9040\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62532167521.6213 - val_loss: 72041889988.6080\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62427279381.8453 - val_loss: 71833011421.1840\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 62266472631.8649 - val_loss: 71976040464.3840\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62406429023.3458 - val_loss: 71866686898.1760\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62209188546.3324 - val_loss: 71765778038.7840\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62125557139.2284 - val_loss: 71570262982.6560\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62137776541.2409 - val_loss: 71515213725.6960\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61999483966.8053 - val_loss: 71504443604.9920\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62190086644.6222 - val_loss: 71397661278.2080\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61848253127.7938 - val_loss: 72308049444.8640\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61770280752.4693 - val_loss: 71292534587.3920\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61576247698.3182 - val_loss: 71315658833.9200\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61501916217.3440 - val_loss: 71312384327.6800\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 61469831907.1004 - val_loss: 71298132672.5120\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 61330277316.8356 - val_loss: 72447079874.5600\n",
      "BEST ACTIVATION FUNCTION relu WITH SCORE 0.7232737253872559\n"
     ]
    }
   ],
   "source": [
    "best_score= 1000.0 #bad\n",
    "\n",
    "#loop over activation functions, test on valid, take model with best results\n",
    "for activ in ['sigmoid','tanh','relu']:\n",
    "    model = nn_model(activation=activ)\n",
    "\n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                epochs=50, batch_size=128,\n",
    "                validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "\n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "\n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title(\"training curve\")\n",
    "    plt.plot(hist.history['loss'], label= 'train')\n",
    "    plt.plot(hist.history['val_loss'], label= 'val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mse')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.158106372019069e-17\n",
      "1.0\n",
      "0.016119707489855875\n",
      "0.9970087526331801\n",
      "0.09330835574898895\n",
      "0.9414925643047213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler to the training set and perform the transformation\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "# Use the fitted scaler to transform validation and test features\n",
    "selected_feature_val = in_scaler.transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.transform(selected_feature_test)\n",
    "\n",
    "# Check appropriate scaling\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_val[:,0]))\n",
    "print(np.std(selected_feature_val[:,0]))\n",
    "\n",
    "print(np.mean(selected_feature_test[:,0]))\n",
    "print(np.std(selected_feature_test[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 416644576342.4711 - val_loss: 456238560968.7040\n",
      "Epoch 2/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416217345847.7511 - val_loss: 455239404617.7280\n",
      "Epoch 3/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 414568203652.6649 - val_loss: 452390604242.9440\n",
      "Epoch 4/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 410827554699.4915 - val_loss: 446754436153.3439\n",
      "Epoch 5/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 404285346603.9182 - val_loss: 437735351123.9680\n",
      "Epoch 6/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 394443159881.5004 - val_loss: 424681049686.0160\n",
      "Epoch 7/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 380969343341.9094 - val_loss: 407551324192.7680\n",
      "Epoch 8/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 363719509715.6266 - val_loss: 386206234312.7040\n",
      "Epoch 9/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 342901525694.2365 - val_loss: 361187880730.6240\n",
      "Epoch 10/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 319174981946.9369 - val_loss: 333358503297.0240\n",
      "Epoch 11/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 293264626373.9733 - val_loss: 303675326595.0720\n",
      "Epoch 12/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 265975447450.0551 - val_loss: 272757238792.1920\n",
      "Epoch 13/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 238384875743.9146 - val_loss: 242220663963.6480\n",
      "Epoch 14/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 211418198281.7849 - val_loss: 213141047476.2240\n",
      "Epoch 15/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 186103864958.0658 - val_loss: 186206458740.7360\n",
      "Epoch 16/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 163120559845.8311 - val_loss: 162286802698.2400\n",
      "Epoch 17/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 143141775015.9360 - val_loss: 141998807056.3840\n",
      "Epoch 18/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 126342424079.0187 - val_loss: 125546654662.6560\n",
      "Epoch 19/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 112746185111.7796 - val_loss: 112486761168.8960\n",
      "Epoch 20/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 102037485737.3013 - val_loss: 102660495638.5280\n",
      "Epoch 21/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 93891722346.4960 - val_loss: 95132374204.4160\n",
      "Epoch 22/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 87840075197.0987 - val_loss: 89750599892.9920\n",
      "Epoch 23/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 83350682970.7947 - val_loss: 85775138750.4640\n",
      "Epoch 24/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 80011494697.6427 - val_loss: 82836923613.1840\n",
      "Epoch 25/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 77462002579.6836 - val_loss: 80537609437.1840\n",
      "Epoch 26/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 75461798250.2684 - val_loss: 78715300478.9760\n",
      "Epoch 27/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 73832137459.4845 - val_loss: 77193511108.6080\n",
      "Epoch 28/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 72433476588.8853 - val_loss: 75879739817.9840\n",
      "Epoch 29/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 71215732333.6818 - val_loss: 74691017834.4960\n",
      "Epoch 30/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 70127951402.3253 - val_loss: 73618587713.5360\n",
      "Epoch 31/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 69132991295.9431 - val_loss: 72635948957.6960\n",
      "Epoch 32/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 68205281720.5476 - val_loss: 71703676092.4160\n",
      "Epoch 33/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 67319199895.0969 - val_loss: 70820602052.6080\n",
      "Epoch 34/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 66473161428.5369 - val_loss: 69966396588.0320\n",
      "Epoch 35/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 65656857835.7476 - val_loss: 69142880157.6960\n",
      "Epoch 36/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 64870828791.1253 - val_loss: 68336612868.0960\n",
      "Epoch 37/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 64091100632.4053 - val_loss: 67556197007.3600\n",
      "Epoch 38/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 63337537819.9893 - val_loss: 66789644632.0640\n",
      "Epoch 39/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 62583989090.5316 - val_loss: 66039014948.8640\n",
      "Epoch 40/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 61850176866.0764 - val_loss: 65278668144.6400\n",
      "Epoch 41/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61108478916.8356 - val_loss: 64544617594.8800\n",
      "Epoch 42/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 60380982389.4187 - val_loss: 63804028747.7760\n",
      "Epoch 43/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 59655526383.6160 - val_loss: 63080853667.8400\n",
      "Epoch 44/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 58934420122.2827 - val_loss: 62360027725.8240\n",
      "Epoch 45/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 58211417893.5467 - val_loss: 61645425180.6720\n",
      "Epoch 46/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 57501619861.7316 - val_loss: 60933677744.1280\n",
      "Epoch 47/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 56790034555.7902 - val_loss: 60217458065.4080\n",
      "Epoch 48/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 56075638174.1511 - val_loss: 59510826237.9520\n",
      "Epoch 49/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 55378959990.7840 - val_loss: 58813862969.3440\n",
      "Epoch 50/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 54661660668.3591 - val_loss: 58114302738.4320\n",
      "Epoch 51/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 53964358877.1840 - val_loss: 57422060584.9600\n",
      "Epoch 52/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 53263661334.5280 - val_loss: 56735982649.3440\n",
      "Epoch 53/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 52570159295.1467 - val_loss: 56056375934.9760\n",
      "Epoch 54/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 51867418234.4249 - val_loss: 55378855952.3840\n",
      "Epoch 55/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 51185292476.4160 - val_loss: 54715634188.2880\n",
      "Epoch 56/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 50502254402.6738 - val_loss: 54055708360.7040\n",
      "Epoch 57/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 49828152522.9796 - val_loss: 53411091611.6480\n",
      "Epoch 58/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 49152976515.5271 - val_loss: 52764073295.8720\n",
      "Epoch 59/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 48487888333.4827 - val_loss: 52126332944.3840\n",
      "Epoch 60/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 47820751303.1111 - val_loss: 51499287642.1120\n",
      "Epoch 61/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 47184737861.6320 - val_loss: 50891294244.8640\n",
      "Epoch 62/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 46530923094.9262 - val_loss: 50293033533.4400\n",
      "Epoch 63/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 45901191417.4009 - val_loss: 49703395885.0560\n",
      "Epoch 64/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 45274278071.8649 - val_loss: 49122106376.1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 44668920798.3218 - val_loss: 48556024594.4320\n",
      "Epoch 66/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 44065825944.0071 - val_loss: 48018722324.4800\n",
      "Epoch 67/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 43491713708.4871 - val_loss: 47489612972.0320\n",
      "Epoch 68/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 42933053449.1022 - val_loss: 46974494277.6320\n",
      "Epoch 69/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 42380851664.2133 - val_loss: 46489537085.4400\n",
      "Epoch 70/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 41848715061.9307 - val_loss: 46021432279.0400\n",
      "Epoch 71/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 41358062150.5422 - val_loss: 45597235871.7440\n",
      "Epoch 72/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40881822851.9822 - val_loss: 45192002076.6720\n",
      "Epoch 73/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40442540108.4587 - val_loss: 44828175269.8880\n",
      "Epoch 74/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40032679315.2284 - val_loss: 44484250107.9040\n",
      "Epoch 75/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 39653459831.4667 - val_loss: 44179962265.6000\n",
      "Epoch 76/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 39298214526.9760 - val_loss: 43898074923.0080\n",
      "Epoch 77/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38974269288.9031 - val_loss: 43653711298.5600\n",
      "Epoch 78/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 38674781929.4720 - val_loss: 43426748039.1680\n",
      "Epoch 79/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38406782653.7813 - val_loss: 43210602315.7760\n",
      "Epoch 80/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 38148313572.2382 - val_loss: 43036721741.8240\n",
      "Epoch 81/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37919998419.8542 - val_loss: 42874549010.4320\n",
      "Epoch 82/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37711136926.3787 - val_loss: 42726335578.1120\n",
      "Epoch 83/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37508381203.5698 - val_loss: 42557714792.4480\n",
      "Epoch 84/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37333680754.2329 - val_loss: 42437671550.9760\n",
      "Epoch 85/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37167447563.3778 - val_loss: 42306841411.5840\n",
      "Epoch 86/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 37004177775.7298 - val_loss: 42180146069.5040\n",
      "Epoch 87/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36849783275.5200 - val_loss: 42060031787.0080\n",
      "Epoch 88/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36715287571.1147 - val_loss: 41921592492.0320\n",
      "Epoch 89/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36580846876.8996 - val_loss: 41838888714.2400\n",
      "Epoch 90/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36448254776.6613 - val_loss: 41759994609.6640\n",
      "Epoch 91/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36334534306.4747 - val_loss: 41634331426.8160\n",
      "Epoch 92/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36216975840.5973 - val_loss: 41543634780.1600\n",
      "Epoch 93/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36104797745.6071 - val_loss: 41463151984.6400\n",
      "Epoch 94/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35995388834.2471 - val_loss: 41350358564.8640\n",
      "Epoch 95/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35895702534.3716 - val_loss: 41278537007.1040\n",
      "Epoch 96/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35785852241.6924 - val_loss: 41165477838.8480\n",
      "Epoch 97/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35698304090.1120 - val_loss: 41113439895.5520\n",
      "Epoch 98/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35598446289.8062 - val_loss: 41015305273.3440\n",
      "Epoch 99/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35509605657.2587 - val_loss: 40898399240.1920\n",
      "Epoch 100/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35411737204.0533 - val_loss: 40823419895.8080\n",
      "Epoch 101/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35329799822.4498 - val_loss: 40729715212.2880\n",
      "Epoch 102/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35236185919.0329 - val_loss: 40650722836.4800\n",
      "Epoch 103/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35161361567.2889 - val_loss: 40573002547.2000\n",
      "Epoch 104/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35077831406.0231 - val_loss: 40494818361.3440\n",
      "Epoch 105/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34997146482.9156 - val_loss: 40411774681.0880\n",
      "Epoch 106/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34915281079.8649 - val_loss: 40333475643.3920\n",
      "Epoch 107/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34842355863.0969 - val_loss: 40278458236.9280\n",
      "Epoch 108/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34752979559.3102 - val_loss: 40194097971.2000\n",
      "Epoch 109/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34686601023.0329 - val_loss: 40110877704.1920\n",
      "Epoch 110/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34623551380.5938 - val_loss: 40076369526.7840\n",
      "Epoch 111/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 34540863568.0996 - val_loss: 39974440894.4640\n",
      "Epoch 112/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34469964853.7031 - val_loss: 39914965991.4240\n",
      "Epoch 113/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34410551731.0862 - val_loss: 39844577705.9840\n",
      "Epoch 114/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34347353261.8524 - val_loss: 39765935259.6480\n",
      "Epoch 115/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34284398987.0364 - val_loss: 39721119744.0000\n",
      "Epoch 116/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34220234171.2782 - val_loss: 39665116119.0400\n",
      "Epoch 117/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34161066169.6853 - val_loss: 39592611086.3360\n",
      "Epoch 118/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34105843835.7902 - val_loss: 39519580454.9120\n",
      "Epoch 119/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34050038369.8489 - val_loss: 39442955206.6560\n",
      "Epoch 120/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33998128174.4213 - val_loss: 39433035710.4640\n",
      "Epoch 121/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33945009462.3858 - val_loss: 39390726258.6880\n",
      "Epoch 122/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33892260593.6640 - val_loss: 39290788216.8320\n",
      "Epoch 123/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33845364426.5244 - val_loss: 39231196561.4080\n",
      "Epoch 124/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33801335561.3298 - val_loss: 39178487529.4720\n",
      "Epoch 125/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33748454655.7724 - val_loss: 39172306763.7760\n",
      "Epoch 126/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33703897936.3271 - val_loss: 39108091674.6240\n",
      "Epoch 127/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33661378907.7049 - val_loss: 39036133081.0880\n",
      "Epoch 128/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33616910086.5991 - val_loss: 39011487219.7120\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 12us/step - loss: 33574015460.2382 - val_loss: 38947042656.2560\n",
      "Epoch 130/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33535860282.7093 - val_loss: 38937162612.7360\n",
      "Epoch 131/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33509337834.3822 - val_loss: 38841020252.1600\n",
      "Epoch 132/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33451758348.9707 - val_loss: 38827133960.1920\n",
      "Epoch 133/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33413189125.9164 - val_loss: 38790277922.8160\n",
      "Epoch 134/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33370294787.1858 - val_loss: 38740267368.4480\n",
      "Epoch 135/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33336932197.2622 - val_loss: 38691618455.5520\n",
      "Epoch 136/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33301094136.0356 - val_loss: 38672344088.5760\n",
      "Epoch 137/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33266636752.6684 - val_loss: 38617681199.1040\n",
      "Epoch 138/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33228555441.4933 - val_loss: 38581107720.1920\n",
      "Epoch 139/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33194025415.1111 - val_loss: 38531656646.6560\n",
      "Epoch 140/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33158570439.1111 - val_loss: 38478931886.0800\n",
      "Epoch 141/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33126362335.9147 - val_loss: 38467354886.1440\n",
      "Epoch 142/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33096777871.8151 - val_loss: 38421500854.2720\n",
      "Epoch 143/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33069021386.0693 - val_loss: 38372189700.0960\n",
      "Epoch 144/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33032002495.3742 - val_loss: 38349326778.3680\n",
      "Epoch 145/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 33010681293.4827 - val_loss: 38300859170.8160\n",
      "Epoch 146/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32971567174.9973 - val_loss: 38269836427.2640\n",
      "Epoch 147/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32944713127.2533 - val_loss: 38261004238.8480\n",
      "Epoch 148/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32912753499.2498 - val_loss: 38224791961.6000\n",
      "Epoch 149/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32890911945.1591 - val_loss: 38178324316.1600\n",
      "Epoch 150/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32859337545.0453 - val_loss: 38168772476.9280\n",
      "Epoch 151/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32831836551.3956 - val_loss: 38145527808.0000\n",
      "Epoch 152/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32802632703.0898 - val_loss: 38084730748.9280\n",
      "Epoch 153/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32783978789.0916 - val_loss: 38069544222.7200\n",
      "Epoch 154/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32754443255.8080 - val_loss: 38056471789.5680\n",
      "Epoch 155/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32730641685.6178 - val_loss: 38014046109.6960\n",
      "Epoch 156/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32711920557.1698 - val_loss: 37993511714.8160\n",
      "Epoch 157/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32681598852.2098 - val_loss: 37955694690.3040\n",
      "Epoch 158/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32654966430.8338 - val_loss: 37904729276.4160\n",
      "Epoch 159/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32631888419.9538 - val_loss: 37889563361.2800\n",
      "Epoch 160/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32606680117.7031 - val_loss: 37865541894.1440\n",
      "Epoch 161/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32592057597.0418 - val_loss: 37825034649.6000\n",
      "Epoch 162/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32568141277.8667 - val_loss: 37833672753.1520\n",
      "Epoch 163/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32540324418.9013 - val_loss: 37801226764.2880\n",
      "Epoch 164/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32513241113.4862 - val_loss: 37767450427.3920\n",
      "Epoch 165/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32498690950.9404 - val_loss: 37737614082.0480\n",
      "Epoch 166/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32467629675.8613 - val_loss: 37738618748.9280\n",
      "Epoch 167/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32461553920.6827 - val_loss: 37692901785.6000\n",
      "Epoch 168/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32428972399.7298 - val_loss: 37678381465.6000\n",
      "Epoch 169/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32408441156.0391 - val_loss: 37661774053.3760\n",
      "Epoch 170/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32395490275.7831 - val_loss: 37609457549.3120\n",
      "Epoch 171/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32372817069.8524 - val_loss: 37575449051.1360\n",
      "Epoch 172/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32352054271.0898 - val_loss: 37578299473.9200\n",
      "Epoch 173/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32322665325.4542 - val_loss: 37548080955.3920\n",
      "Epoch 174/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32313483052.8284 - val_loss: 37540394663.9360\n",
      "Epoch 175/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32299566081.8204 - val_loss: 37467916271.6160\n",
      "Epoch 176/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32275629393.6924 - val_loss: 37508011065.3440\n",
      "Epoch 177/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32259084312.5760 - val_loss: 37455414919.1680\n",
      "Epoch 178/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32230055204.1813 - val_loss: 37463063003.1360\n",
      "Epoch 179/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32212277293.5111 - val_loss: 37411546398.7200\n",
      "Epoch 180/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32190198901.4187 - val_loss: 37391206416.3840\n",
      "Epoch 181/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32170211952.4124 - val_loss: 37382184828.9280\n",
      "Epoch 182/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32155861687.4098 - val_loss: 37382649643.0080\n",
      "Epoch 183/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32138743693.3120 - val_loss: 37343953649.6640\n",
      "Epoch 184/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32120202009.7138 - val_loss: 37360779853.8240\n",
      "Epoch 185/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32106757597.8667 - val_loss: 37345048100.8640\n",
      "Epoch 186/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32082446137.5716 - val_loss: 37283556032.5120\n",
      "Epoch 187/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32059921021.1556 - val_loss: 37278082203.6480\n",
      "Epoch 188/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32055404047.0187 - val_loss: 37280353681.4080\n",
      "Epoch 189/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32024480461.2551 - val_loss: 37231869329.4080\n",
      "Epoch 190/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32005773743.4453 - val_loss: 37219087613.9520\n",
      "Epoch 191/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31985720561.2089 - val_loss: 37181888495.6160\n",
      "Epoch 192/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31965186501.2907 - val_loss: 37190197739.5200\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 13us/step - loss: 31949070141.2124 - val_loss: 37163080417.2800\n",
      "Epoch 194/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31931307527.7369 - val_loss: 37145755058.1760\n",
      "Epoch 195/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31915058033.0951 - val_loss: 37132860424.1920\n",
      "Epoch 196/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31906238343.8507 - val_loss: 37122890137.6000\n",
      "Epoch 197/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 31893298808.6044 - val_loss: 37079881121.7920\n",
      "Epoch 198/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31859803912.4196 - val_loss: 37084478504.9600\n",
      "Epoch 199/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31836456517.6320 - val_loss: 37047174922.2400\n",
      "Epoch 200/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31823952594.7164 - val_loss: 37023406489.6000\n",
      "0.6648199703919383\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=200, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/400\n",
      "18000/18000 [==============================] - 1s 63us/step - loss: 416651191954.0906 - val_loss: 456254929764.3519\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456254929764.35199, saving model to best_model.h5\n",
      "Epoch 2/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416239840977.8062 - val_loss: 455212861227.0080\n",
      "\n",
      "Epoch 00002: val_loss improved from 456254929764.35199 to 455212861227.00800, saving model to best_model.h5\n",
      "Epoch 3/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 414451035653.9164 - val_loss: 451995516796.9279\n",
      "\n",
      "Epoch 00003: val_loss improved from 455212861227.00800 to 451995516796.92798, saving model to best_model.h5\n",
      "Epoch 4/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 410312852548.2667 - val_loss: 445569458241.5360\n",
      "\n",
      "Epoch 00004: val_loss improved from 451995516796.92798 to 445569458241.53601, saving model to best_model.h5\n",
      "Epoch 5/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 403103493419.4631 - val_loss: 435408450355.2000\n",
      "\n",
      "Epoch 00005: val_loss improved from 445569458241.53601 to 435408450355.20001, saving model to best_model.h5\n",
      "Epoch 6/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 392284258959.3600 - val_loss: 420892888268.8000\n",
      "\n",
      "Epoch 00006: val_loss improved from 435408450355.20001 to 420892888268.79999, saving model to best_model.h5\n",
      "Epoch 7/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 377690813701.2338 - val_loss: 402099762626.5600\n",
      "\n",
      "Epoch 00007: val_loss improved from 420892888268.79999 to 402099762626.56000, saving model to best_model.h5\n",
      "Epoch 8/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 359454168259.6978 - val_loss: 379327575490.5600\n",
      "\n",
      "Epoch 00008: val_loss improved from 402099762626.56000 to 379327575490.56000, saving model to best_model.h5\n",
      "Epoch 9/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 337897688050.3467 - val_loss: 353187719020.5440\n",
      "\n",
      "Epoch 00009: val_loss improved from 379327575490.56000 to 353187719020.54401, saving model to best_model.h5\n",
      "Epoch 10/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 313705834916.5227 - val_loss: 324362766647.2960\n",
      "\n",
      "Epoch 00010: val_loss improved from 353187719020.54401 to 324362766647.29602, saving model to best_model.h5\n",
      "Epoch 11/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 287670560882.6880 - val_loss: 294244664213.5040\n",
      "\n",
      "Epoch 00011: val_loss improved from 324362766647.29602 to 294244664213.50403, saving model to best_model.h5\n",
      "Epoch 12/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 260943950075.2213 - val_loss: 264101729009.6640\n",
      "\n",
      "Epoch 00012: val_loss improved from 294244664213.50403 to 264101729009.66400, saving model to best_model.h5\n",
      "Epoch 13/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 234809822281.7280 - val_loss: 235032811929.6000\n",
      "\n",
      "Epoch 00013: val_loss improved from 264101729009.66400 to 235032811929.60001, saving model to best_model.h5\n",
      "Epoch 14/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 210066257768.9031 - val_loss: 208383032033.2800\n",
      "\n",
      "Epoch 00014: val_loss improved from 235032811929.60001 to 208383032033.28000, saving model to best_model.h5\n",
      "Epoch 15/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 187583977167.9858 - val_loss: 184700689973.2480\n",
      "\n",
      "Epoch 00015: val_loss improved from 208383032033.28000 to 184700689973.24799, saving model to best_model.h5\n",
      "Epoch 16/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 167914770624.9671 - val_loss: 164529266622.4640\n",
      "\n",
      "Epoch 00016: val_loss improved from 184700689973.24799 to 164529266622.46399, saving model to best_model.h5\n",
      "Epoch 17/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 151233280882.9155 - val_loss: 148092952248.3200\n",
      "\n",
      "Epoch 00017: val_loss improved from 164529266622.46399 to 148092952248.32001, saving model to best_model.h5\n",
      "Epoch 18/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 137410233485.0845 - val_loss: 134771468271.6160\n",
      "\n",
      "Epoch 00018: val_loss improved from 148092952248.32001 to 134771468271.61600, saving model to best_model.h5\n",
      "Epoch 19/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 126152282720.0285 - val_loss: 124307040829.4400\n",
      "\n",
      "Epoch 00019: val_loss improved from 134771468271.61600 to 124307040829.44000, saving model to best_model.h5\n",
      "Epoch 20/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 117144603095.4951 - val_loss: 116117517828.0960\n",
      "\n",
      "Epoch 00020: val_loss improved from 124307040829.44000 to 116117517828.09599, saving model to best_model.h5\n",
      "Epoch 21/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 109944048655.4738 - val_loss: 109677868613.6320\n",
      "\n",
      "Epoch 00021: val_loss improved from 116117517828.09599 to 109677868613.63200, saving model to best_model.h5\n",
      "Epoch 22/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 104002604262.2862 - val_loss: 104429930676.2240\n",
      "\n",
      "Epoch 00022: val_loss improved from 109677868613.63200 to 104429930676.22400, saving model to best_model.h5\n",
      "Epoch 23/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 99036519515.9324 - val_loss: 100019792379.9040\n",
      "\n",
      "Epoch 00023: val_loss improved from 104429930676.22400 to 100019792379.90401, saving model to best_model.h5\n",
      "Epoch 24/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 94760018487.9787 - val_loss: 96177436557.3120\n",
      "\n",
      "Epoch 00024: val_loss improved from 100019792379.90401 to 96177436557.31200, saving model to best_model.h5\n",
      "Epoch 25/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 90970908043.9467 - val_loss: 92770877571.0720\n",
      "\n",
      "Epoch 00025: val_loss improved from 96177436557.31200 to 92770877571.07201, saving model to best_model.h5\n",
      "Epoch 26/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 87571079379.1716 - val_loss: 89681957486.5920\n",
      "\n",
      "Epoch 00026: val_loss improved from 92770877571.07201 to 89681957486.59200, saving model to best_model.h5\n",
      "Epoch 27/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 84514678672.9529 - val_loss: 86850505211.9040\n",
      "\n",
      "Epoch 00027: val_loss improved from 89681957486.59200 to 86850505211.90401, saving model to best_model.h5\n",
      "Epoch 28/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 81759725799.1964 - val_loss: 84309185986.5600\n",
      "\n",
      "Epoch 00028: val_loss improved from 86850505211.90401 to 84309185986.56000, saving model to best_model.h5\n",
      "Epoch 29/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 79297489717.0204 - val_loss: 81986510913.5360\n",
      "\n",
      "Epoch 00029: val_loss improved from 84309185986.56000 to 81986510913.53600, saving model to best_model.h5\n",
      "Epoch 30/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 77095235725.9947 - val_loss: 79909887410.1760\n",
      "\n",
      "Epoch 00030: val_loss improved from 81986510913.53600 to 79909887410.17599, saving model to best_model.h5\n",
      "Epoch 31/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 75138606213.8027 - val_loss: 78025085616.1280\n",
      "\n",
      "Epoch 00031: val_loss improved from 79909887410.17599 to 78025085616.12801, saving model to best_model.h5\n",
      "Epoch 32/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 73381407038.5778 - val_loss: 76354847113.2160\n",
      "\n",
      "Epoch 00032: val_loss improved from 78025085616.12801 to 76354847113.21600, saving model to best_model.h5\n",
      "Epoch 33/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 71840651542.5280 - val_loss: 74830658404.3520\n",
      "\n",
      "Epoch 00033: val_loss improved from 76354847113.21600 to 74830658404.35201, saving model to best_model.h5\n",
      "Epoch 34/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 70465175005.8667 - val_loss: 73488728129.5360\n",
      "\n",
      "Epoch 00034: val_loss improved from 74830658404.35201 to 73488728129.53600, saving model to best_model.h5\n",
      "Epoch 35/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 69253871705.2018 - val_loss: 72269803290.6240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss improved from 73488728129.53600 to 72269803290.62399, saving model to best_model.h5\n",
      "Epoch 36/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 68136297693.1840 - val_loss: 71153654890.4960\n",
      "\n",
      "Epoch 00036: val_loss improved from 72269803290.62399 to 71153654890.49600, saving model to best_model.h5\n",
      "Epoch 37/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 67135716007.0258 - val_loss: 70148414570.4960\n",
      "\n",
      "Epoch 00037: val_loss improved from 71153654890.49600 to 70148414570.49600, saving model to best_model.h5\n",
      "Epoch 38/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 66225098864.8676 - val_loss: 69202517983.2320\n",
      "\n",
      "Epoch 00038: val_loss improved from 70148414570.49600 to 69202517983.23199, saving model to best_model.h5\n",
      "Epoch 39/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 65341304507.9609 - val_loss: 68319505514.4960\n",
      "\n",
      "Epoch 00039: val_loss improved from 69202517983.23199 to 68319505514.49600, saving model to best_model.h5\n",
      "Epoch 40/400\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 64528228657.8347 - val_loss: 67461973245.9520\n",
      "\n",
      "Epoch 00040: val_loss improved from 68319505514.49600 to 67461973245.95200, saving model to best_model.h5\n",
      "Epoch 41/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 63718296973.7671 - val_loss: 66669834436.6080\n",
      "\n",
      "Epoch 00041: val_loss improved from 67461973245.95200 to 66669834436.60800, saving model to best_model.h5\n",
      "Epoch 42/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62956298323.7404 - val_loss: 65876250918.9120\n",
      "\n",
      "Epoch 00042: val_loss improved from 66669834436.60800 to 65876250918.91200, saving model to best_model.h5\n",
      "Epoch 43/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 62197416565.8738 - val_loss: 65121297301.5040\n",
      "\n",
      "Epoch 00043: val_loss improved from 65876250918.91200 to 65121297301.50400, saving model to best_model.h5\n",
      "Epoch 44/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 61444809308.3876 - val_loss: 64376546328.5760\n",
      "\n",
      "Epoch 00044: val_loss improved from 65121297301.50400 to 64376546328.57600, saving model to best_model.h5\n",
      "Epoch 45/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 60712340684.8000 - val_loss: 63652508794.8800\n",
      "\n",
      "Epoch 00045: val_loss improved from 64376546328.57600 to 63652508794.88000, saving model to best_model.h5\n",
      "Epoch 46/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 59963602538.0409 - val_loss: 62918686375.9360\n",
      "\n",
      "Epoch 00046: val_loss improved from 63652508794.88000 to 62918686375.93600, saving model to best_model.h5\n",
      "Epoch 47/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 59228431959.8364 - val_loss: 62212526473.2160\n",
      "\n",
      "Epoch 00047: val_loss improved from 62918686375.93600 to 62212526473.21600, saving model to best_model.h5\n",
      "Epoch 48/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 58504595017.2729 - val_loss: 61476848631.8080\n",
      "\n",
      "Epoch 00048: val_loss improved from 62212526473.21600 to 61476848631.80800, saving model to best_model.h5\n",
      "Epoch 49/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 57766796729.4578 - val_loss: 60771088662.5280\n",
      "\n",
      "Epoch 00049: val_loss improved from 61476848631.80800 to 60771088662.52800, saving model to best_model.h5\n",
      "Epoch 50/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 57031082428.1884 - val_loss: 60069561106.4320\n",
      "\n",
      "Epoch 00050: val_loss improved from 60771088662.52800 to 60069561106.43200, saving model to best_model.h5\n",
      "Epoch 51/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 56301290939.2782 - val_loss: 59361872871.4240\n",
      "\n",
      "Epoch 00051: val_loss improved from 60069561106.43200 to 59361872871.42400, saving model to best_model.h5\n",
      "Epoch 52/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 55576540457.6427 - val_loss: 58662367526.9120\n",
      "\n",
      "Epoch 00052: val_loss improved from 59361872871.42400 to 58662367526.91200, saving model to best_model.h5\n",
      "Epoch 53/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 54861156865.3653 - val_loss: 57972524810.2400\n",
      "\n",
      "Epoch 00053: val_loss improved from 58662367526.91200 to 57972524810.24000, saving model to best_model.h5\n",
      "Epoch 54/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 54122777560.8604 - val_loss: 57279429967.8720\n",
      "\n",
      "Epoch 00054: val_loss improved from 57972524810.24000 to 57279429967.87200, saving model to best_model.h5\n",
      "Epoch 55/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 53412619117.4542 - val_loss: 56601689522.1760\n",
      "\n",
      "Epoch 00055: val_loss improved from 57279429967.87200 to 56601689522.17600, saving model to best_model.h5\n",
      "Epoch 56/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 52725544007.9076 - val_loss: 55931197849.6000\n",
      "\n",
      "Epoch 00056: val_loss improved from 56601689522.17600 to 55931197849.60000, saving model to best_model.h5\n",
      "Epoch 57/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 52020407402.4960 - val_loss: 55282536349.6960\n",
      "\n",
      "Epoch 00057: val_loss improved from 55931197849.60000 to 55282536349.69600, saving model to best_model.h5\n",
      "Epoch 58/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 51327786099.5982 - val_loss: 54633905717.2480\n",
      "\n",
      "Epoch 00058: val_loss improved from 55282536349.69600 to 54633905717.24800, saving model to best_model.h5\n",
      "Epoch 59/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 50652131313.4364 - val_loss: 53991319044.0960\n",
      "\n",
      "Epoch 00059: val_loss improved from 54633905717.24800 to 53991319044.09600, saving model to best_model.h5\n",
      "Epoch 60/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 49979581930.6098 - val_loss: 53371580219.3920\n",
      "\n",
      "Epoch 00060: val_loss improved from 53991319044.09600 to 53371580219.39200, saving model to best_model.h5\n",
      "Epoch 61/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 49322586563.4702 - val_loss: 52760861769.7280\n",
      "\n",
      "Epoch 00061: val_loss improved from 53371580219.39200 to 52760861769.72800, saving model to best_model.h5\n",
      "Epoch 62/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 48674924888.0640 - val_loss: 52165607981.0560\n",
      "\n",
      "Epoch 00062: val_loss improved from 52760861769.72800 to 52165607981.05600, saving model to best_model.h5\n",
      "Epoch 63/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 48039774275.3564 - val_loss: 51574205382.6560\n",
      "\n",
      "Epoch 00063: val_loss improved from 52165607981.05600 to 51574205382.65600, saving model to best_model.h5\n",
      "Epoch 64/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 47415778337.6782 - val_loss: 51015434436.6080\n",
      "\n",
      "Epoch 00064: val_loss improved from 51574205382.65600 to 51015434436.60800, saving model to best_model.h5\n",
      "Epoch 65/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 46799645835.2640 - val_loss: 50464578699.2640\n",
      "\n",
      "Epoch 00065: val_loss improved from 51015434436.60800 to 50464578699.26400, saving model to best_model.h5\n",
      "Epoch 66/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 46204631129.2018 - val_loss: 49919864406.0160\n",
      "\n",
      "Epoch 00066: val_loss improved from 50464578699.26400 to 49919864406.01600, saving model to best_model.h5\n",
      "Epoch 67/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 45622930527.5733 - val_loss: 49397410562.0480\n",
      "\n",
      "Epoch 00067: val_loss improved from 49919864406.01600 to 49397410562.04800, saving model to best_model.h5\n",
      "Epoch 68/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 45042773211.3636 - val_loss: 48879799762.9440\n",
      "\n",
      "Epoch 00068: val_loss improved from 49397410562.04800 to 48879799762.94400, saving model to best_model.h5\n",
      "Epoch 69/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 44519904316.9849 - val_loss: 48390926532.6080\n",
      "\n",
      "Epoch 00069: val_loss improved from 48879799762.94400 to 48390926532.60800, saving model to best_model.h5\n",
      "Epoch 70/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 43960825071.3885 - val_loss: 47897013977.0880\n",
      "\n",
      "Epoch 00070: val_loss improved from 48390926532.60800 to 47897013977.08800, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 43455413179.7333 - val_loss: 47444390903.8080\n",
      "\n",
      "Epoch 00071: val_loss improved from 47897013977.08800 to 47444390903.80800, saving model to best_model.h5\n",
      "Epoch 72/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 42957391021.8524 - val_loss: 47000602869.7600\n",
      "\n",
      "Epoch 00072: val_loss improved from 47444390903.80800 to 47000602869.76000, saving model to best_model.h5\n",
      "Epoch 73/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 42468395778.0480 - val_loss: 46586556219.3920\n",
      "\n",
      "Epoch 00073: val_loss improved from 47000602869.76000 to 46586556219.39200, saving model to best_model.h5\n",
      "Epoch 74/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 42015351126.2436 - val_loss: 46177700380.6720\n",
      "\n",
      "Epoch 00074: val_loss improved from 46586556219.39200 to 46177700380.67200, saving model to best_model.h5\n",
      "Epoch 75/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 41554126926.2791 - val_loss: 45770643406.8480\n",
      "\n",
      "Epoch 00075: val_loss improved from 46177700380.67200 to 45770643406.84800, saving model to best_model.h5\n",
      "Epoch 76/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 41129762406.4000 - val_loss: 45392467492.8640\n",
      "\n",
      "Epoch 00076: val_loss improved from 45770643406.84800 to 45392467492.86400, saving model to best_model.h5\n",
      "Epoch 77/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40724831950.1653 - val_loss: 45035664736.2560\n",
      "\n",
      "Epoch 00077: val_loss improved from 45392467492.86400 to 45035664736.25600, saving model to best_model.h5\n",
      "Epoch 78/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 40345123169.1662 - val_loss: 44691523698.6880\n",
      "\n",
      "Epoch 00078: val_loss improved from 45035664736.25600 to 44691523698.68800, saving model to best_model.h5\n",
      "Epoch 79/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 39965685206.5849 - val_loss: 44370367479.8080\n",
      "\n",
      "Epoch 00079: val_loss improved from 44691523698.68800 to 44370367479.80800, saving model to best_model.h5\n",
      "Epoch 80/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 39627719067.4204 - val_loss: 44057708036.0960\n",
      "\n",
      "Epoch 00080: val_loss improved from 44370367479.80800 to 44057708036.09600, saving model to best_model.h5\n",
      "Epoch 81/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 39265814007.3529 - val_loss: 43756082921.4720\n",
      "\n",
      "Epoch 00081: val_loss improved from 44057708036.09600 to 43756082921.47200, saving model to best_model.h5\n",
      "Epoch 82/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 38950782347.9467 - val_loss: 43474163138.5600\n",
      "\n",
      "Epoch 00082: val_loss improved from 43756082921.47200 to 43474163138.56000, saving model to best_model.h5\n",
      "Epoch 83/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38653150498.3609 - val_loss: 43219343572.9920\n",
      "\n",
      "Epoch 00083: val_loss improved from 43474163138.56000 to 43219343572.99200, saving model to best_model.h5\n",
      "Epoch 84/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38355638987.4347 - val_loss: 42959438675.9680\n",
      "\n",
      "Epoch 00084: val_loss improved from 43219343572.99200 to 42959438675.96800, saving model to best_model.h5\n",
      "Epoch 85/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38094213336.6329 - val_loss: 42717072621.5680\n",
      "\n",
      "Epoch 00085: val_loss improved from 42959438675.96800 to 42717072621.56800, saving model to best_model.h5\n",
      "Epoch 86/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37834885962.8658 - val_loss: 42512652173.3120\n",
      "\n",
      "Epoch 00086: val_loss improved from 42717072621.56800 to 42512652173.31200, saving model to best_model.h5\n",
      "Epoch 87/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37588252116.7644 - val_loss: 42305291812.8640\n",
      "\n",
      "Epoch 00087: val_loss improved from 42512652173.31200 to 42305291812.86400, saving model to best_model.h5\n",
      "Epoch 88/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37366221884.9849 - val_loss: 42123887673.3440\n",
      "\n",
      "Epoch 00088: val_loss improved from 42305291812.86400 to 42123887673.34400, saving model to best_model.h5\n",
      "Epoch 89/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 37149674797.2836 - val_loss: 41913625509.8880\n",
      "\n",
      "Epoch 00089: val_loss improved from 42123887673.34400 to 41913625509.88800, saving model to best_model.h5\n",
      "Epoch 90/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36947930542.5351 - val_loss: 41734978961.4080\n",
      "\n",
      "Epoch 00090: val_loss improved from 41913625509.88800 to 41734978961.40800, saving model to best_model.h5\n",
      "Epoch 91/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36760898329.7138 - val_loss: 41571407790.0800\n",
      "\n",
      "Epoch 00091: val_loss improved from 41734978961.40800 to 41571407790.08000, saving model to best_model.h5\n",
      "Epoch 92/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36577350545.8631 - val_loss: 41407262523.3920\n",
      "\n",
      "Epoch 00092: val_loss improved from 41571407790.08000 to 41407262523.39200, saving model to best_model.h5\n",
      "Epoch 93/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36422392132.0391 - val_loss: 41285077106.6880\n",
      "\n",
      "Epoch 00093: val_loss improved from 41407262523.39200 to 41285077106.68800, saving model to best_model.h5\n",
      "Epoch 94/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36263353903.7867 - val_loss: 41140084178.9440\n",
      "\n",
      "Epoch 00094: val_loss improved from 41285077106.68800 to 41140084178.94400, saving model to best_model.h5\n",
      "Epoch 95/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36120714765.1982 - val_loss: 41012497416.1920\n",
      "\n",
      "Epoch 00095: val_loss improved from 41140084178.94400 to 41012497416.19200, saving model to best_model.h5\n",
      "Epoch 96/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35989313430.4142 - val_loss: 40906163027.9680\n",
      "\n",
      "Epoch 00096: val_loss improved from 41012497416.19200 to 40906163027.96800, saving model to best_model.h5\n",
      "Epoch 97/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35859484413.4969 - val_loss: 40800536723.4560\n",
      "\n",
      "Epoch 00097: val_loss improved from 40906163027.96800 to 40800536723.45600, saving model to best_model.h5\n",
      "Epoch 98/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35738712772.1529 - val_loss: 40686988361.7280\n",
      "\n",
      "Epoch 00098: val_loss improved from 40800536723.45600 to 40686988361.72800, saving model to best_model.h5\n",
      "Epoch 99/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35632283893.7600 - val_loss: 40606294409.2160\n",
      "\n",
      "Epoch 00099: val_loss improved from 40686988361.72800 to 40606294409.21600, saving model to best_model.h5\n",
      "Epoch 100/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35507273593.2871 - val_loss: 40504517459.9680\n",
      "\n",
      "Epoch 00100: val_loss improved from 40606294409.21600 to 40504517459.96800, saving model to best_model.h5\n",
      "Epoch 101/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35409584332.8000 - val_loss: 40413253926.9120\n",
      "\n",
      "Epoch 00101: val_loss improved from 40504517459.96800 to 40413253926.91200, saving model to best_model.h5\n",
      "Epoch 102/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 35315211425.1093 - val_loss: 40348187131.9040\n",
      "\n",
      "Epoch 00102: val_loss improved from 40413253926.91200 to 40348187131.90400, saving model to best_model.h5\n",
      "Epoch 103/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35225935537.0382 - val_loss: 40263508066.3040\n",
      "\n",
      "Epoch 00103: val_loss improved from 40348187131.90400 to 40263508066.30400, saving model to best_model.h5\n",
      "Epoch 104/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35140935629.0276 - val_loss: 40196075094.0160\n",
      "\n",
      "Epoch 00104: val_loss improved from 40263508066.30400 to 40196075094.01600, saving model to best_model.h5\n",
      "Epoch 105/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 35071304231.5947 - val_loss: 40106534633.4720\n",
      "\n",
      "Epoch 00105: val_loss improved from 40196075094.01600 to 40106534633.47200, saving model to best_model.h5\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 14us/step - loss: 34991461032.8462 - val_loss: 40072965226.4960\n",
      "\n",
      "Epoch 00106: val_loss improved from 40106534633.47200 to 40072965226.49600, saving model to best_model.h5\n",
      "Epoch 107/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34912890018.9298 - val_loss: 40011368431.6160\n",
      "\n",
      "Epoch 00107: val_loss improved from 40072965226.49600 to 40011368431.61600, saving model to best_model.h5\n",
      "Epoch 108/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34848528294.7982 - val_loss: 39953021403.1360\n",
      "\n",
      "Epoch 00108: val_loss improved from 40011368431.61600 to 39953021403.13600, saving model to best_model.h5\n",
      "Epoch 109/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34784925450.2400 - val_loss: 39893999648.7680\n",
      "\n",
      "Epoch 00109: val_loss improved from 39953021403.13600 to 39893999648.76800, saving model to best_model.h5\n",
      "Epoch 110/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34727106061.1982 - val_loss: 39854806040.5760\n",
      "\n",
      "Epoch 00110: val_loss improved from 39893999648.76800 to 39854806040.57600, saving model to best_model.h5\n",
      "Epoch 111/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34664856520.4764 - val_loss: 39774199185.4080\n",
      "\n",
      "Epoch 00111: val_loss improved from 39854806040.57600 to 39774199185.40800, saving model to best_model.h5\n",
      "Epoch 112/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34604010708.0818 - val_loss: 39724914343.9360\n",
      "\n",
      "Epoch 00112: val_loss improved from 39774199185.40800 to 39724914343.93600, saving model to best_model.h5\n",
      "Epoch 113/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34554092179.0009 - val_loss: 39682300870.6560\n",
      "\n",
      "Epoch 00113: val_loss improved from 39724914343.93600 to 39682300870.65600, saving model to best_model.h5\n",
      "Epoch 114/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34501406149.2907 - val_loss: 39655165952.0000\n",
      "\n",
      "Epoch 00114: val_loss improved from 39682300870.65600 to 39655165952.00000, saving model to best_model.h5\n",
      "Epoch 115/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34464380798.7484 - val_loss: 39595241177.0880\n",
      "\n",
      "Epoch 00115: val_loss improved from 39655165952.00000 to 39595241177.08800, saving model to best_model.h5\n",
      "Epoch 116/400\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34407233636.1245 - val_loss: 39558512443.3920\n",
      "\n",
      "Epoch 00116: val_loss improved from 39595241177.08800 to 39558512443.39200, saving model to best_model.h5\n",
      "Epoch 117/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34357575392.3698 - val_loss: 39501812858.8800\n",
      "\n",
      "Epoch 00117: val_loss improved from 39558512443.39200 to 39501812858.88000, saving model to best_model.h5\n",
      "Epoch 118/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34320223536.9244 - val_loss: 39481438044.1600\n",
      "\n",
      "Epoch 00118: val_loss improved from 39501812858.88000 to 39481438044.16000, saving model to best_model.h5\n",
      "Epoch 119/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34273618497.0809 - val_loss: 39421742972.9280\n",
      "\n",
      "Epoch 00119: val_loss improved from 39481438044.16000 to 39421742972.92800, saving model to best_model.h5\n",
      "Epoch 120/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34226700675.7547 - val_loss: 39378663440.3840\n",
      "\n",
      "Epoch 00120: val_loss improved from 39421742972.92800 to 39378663440.38400, saving model to best_model.h5\n",
      "Epoch 121/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34187917106.2898 - val_loss: 39356877471.7440\n",
      "\n",
      "Epoch 00121: val_loss improved from 39378663440.38400 to 39356877471.74400, saving model to best_model.h5\n",
      "Epoch 122/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34147926764.2027 - val_loss: 39343226552.3200\n",
      "\n",
      "Epoch 00122: val_loss improved from 39356877471.74400 to 39343226552.32000, saving model to best_model.h5\n",
      "Epoch 123/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34117179887.1609 - val_loss: 39298890792.9600\n",
      "\n",
      "Epoch 00123: val_loss improved from 39343226552.32000 to 39298890792.96000, saving model to best_model.h5\n",
      "Epoch 124/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 34071603547.7049 - val_loss: 39249224499.2000\n",
      "\n",
      "Epoch 00124: val_loss improved from 39298890792.96000 to 39249224499.20000, saving model to best_model.h5\n",
      "Epoch 125/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34036104145.5787 - val_loss: 39199255461.8880\n",
      "\n",
      "Epoch 00125: val_loss improved from 39249224499.20000 to 39199255461.88800, saving model to best_model.h5\n",
      "Epoch 126/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33994146343.5947 - val_loss: 39185198743.5520\n",
      "\n",
      "Epoch 00126: val_loss improved from 39199255461.88800 to 39185198743.55200, saving model to best_model.h5\n",
      "Epoch 127/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33968576739.5556 - val_loss: 39167128240.1280\n",
      "\n",
      "Epoch 00127: val_loss improved from 39185198743.55200 to 39167128240.12800, saving model to best_model.h5\n",
      "Epoch 128/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33927272788.4231 - val_loss: 39105166770.1760\n",
      "\n",
      "Epoch 00128: val_loss improved from 39167128240.12800 to 39105166770.17600, saving model to best_model.h5\n",
      "Epoch 129/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33896290112.8533 - val_loss: 39056889511.9360\n",
      "\n",
      "Epoch 00129: val_loss improved from 39105166770.17600 to 39056889511.93600, saving model to best_model.h5\n",
      "Epoch 130/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33854692427.5484 - val_loss: 39045610930.1760\n",
      "\n",
      "Epoch 00130: val_loss improved from 39056889511.93600 to 39045610930.17600, saving model to best_model.h5\n",
      "Epoch 131/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33828980209.8916 - val_loss: 39005680271.3600\n",
      "\n",
      "Epoch 00131: val_loss improved from 39045610930.17600 to 39005680271.36000, saving model to best_model.h5\n",
      "Epoch 132/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33795271185.7493 - val_loss: 38997329936.3840\n",
      "\n",
      "Epoch 00132: val_loss improved from 39005680271.36000 to 38997329936.38400, saving model to best_model.h5\n",
      "Epoch 133/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33764411313.7209 - val_loss: 38983686881.2800\n",
      "\n",
      "Epoch 00133: val_loss improved from 38997329936.38400 to 38983686881.28000, saving model to best_model.h5\n",
      "Epoch 134/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33730184045.4542 - val_loss: 38944421838.8480\n",
      "\n",
      "Epoch 00134: val_loss improved from 38983686881.28000 to 38944421838.84800, saving model to best_model.h5\n",
      "Epoch 135/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33699312746.4960 - val_loss: 38930392285.1840\n",
      "\n",
      "Epoch 00135: val_loss improved from 38944421838.84800 to 38930392285.18400, saving model to best_model.h5\n",
      "Epoch 136/400\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 33667502669.8240 - val_loss: 38881044168.7040\n",
      "\n",
      "Epoch 00136: val_loss improved from 38930392285.18400 to 38881044168.70400, saving model to best_model.h5\n",
      "Epoch 137/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33635852087.7511 - val_loss: 38844121317.3760\n",
      "\n",
      "Epoch 00137: val_loss improved from 38881044168.70400 to 38844121317.37600, saving model to best_model.h5\n",
      "Epoch 138/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33613292812.5156 - val_loss: 38786496987.1360\n",
      "\n",
      "Epoch 00138: val_loss improved from 38844121317.37600 to 38786496987.13600, saving model to best_model.h5\n",
      "Epoch 139/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33583605117.3831 - val_loss: 38778254589.9520\n",
      "\n",
      "Epoch 00139: val_loss improved from 38786496987.13600 to 38778254589.95200, saving model to best_model.h5\n",
      "Epoch 140/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33558464269.8809 - val_loss: 38762276028.4160\n",
      "\n",
      "Epoch 00140: val_loss improved from 38778254589.95200 to 38762276028.41600, saving model to best_model.h5\n",
      "Epoch 141/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 0s 13us/step - loss: 33526694232.0640 - val_loss: 38717098950.6560\n",
      "\n",
      "Epoch 00141: val_loss improved from 38762276028.41600 to 38717098950.65600, saving model to best_model.h5\n",
      "Epoch 142/400\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 34045597224.778 - 0s 13us/step - loss: 33499196165.6889 - val_loss: 38683190722.5600\n",
      "\n",
      "Epoch 00142: val_loss improved from 38717098950.65600 to 38683190722.56000, saving model to best_model.h5\n",
      "Epoch 143/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33470489136.6969 - val_loss: 38641187127.2960\n",
      "\n",
      "Epoch 00143: val_loss improved from 38683190722.56000 to 38641187127.29600, saving model to best_model.h5\n",
      "Epoch 144/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33440586416.1280 - val_loss: 38672206462.9760\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 38641187127.29600\n",
      "Epoch 145/400\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33417975609.5716 - val_loss: 38650774028.2880\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 38641187127.29600\n",
      "Model score: 0.6834757962274725\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                            monitor='val_loss',\n",
    "                            save_best_only=True,\n",
    "                           verbose=1)]\n",
    "\n",
    "model = nn_model(layers =[20,20,20])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=400, callbacks=callbacks, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
